{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934e5a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mzr0134\\AppData\\Local\\Temp\\ipykernel_11664\\183872592.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\tools\\Anaconda3\\envs\\Blueberry\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from preprocessing import MSC, MeanCenter, Autoscale, trans2absr\n",
    "\n",
    "from SPanalysis import SpectralAnalysis\n",
    "# Image display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "device = torch.device('cpu')\n",
    "run_on_gpu = False\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('yes')\n",
    "    run_on_gpu = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf6aa61c-e580-4926-8303-c636f7e942a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "224a3cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "dataset='dataset\\hyperspectral\\combind.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ba89b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = SpectralAnalysis(dataset,'PLSR')\n",
    "analysis.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e49c8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rwctocat(clss):\n",
    "    if clss== 'Well watered':\n",
    "        return 0\n",
    "    elif clss== 'Mild stress':\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "X_train, y_train_reg, y_train_class, X_test, y_test_reg, y_test_class = analysis.X_train, analysis.y_train_reg.iloc[:,4],analysis.y_train_class,analysis.X_test,analysis.y_test_reg.iloc[:,4],analysis.y_test_class\n",
    "\n",
    "y_train_class = y_train_class.astype('category').cat.codes.to_numpy()\n",
    "y_test_class =  y_test_class.astype('category').cat.codes.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d592fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 2, 1, 0, 2, 2, 0, 1, 1, 1, 0, 2, 0, 2, 1, 0, 2, 1, 2, 0,\n",
       "       0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 1, 1, 0, 2, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 2, 0, 2, 0, 2, 1, 1, 2, 1, 2, 1, 0, 2, 0, 2, 1, 2, 1, 2, 0,\n",
       "       1, 1, 1, 0, 1, 2, 2, 1, 1, 1, 2, 2, 0, 2, 2, 2, 1, 2, 1, 1, 0, 1,\n",
       "       1, 2, 2, 1, 1, 0, 0, 0, 1, 0, 2, 1, 1, 1, 1, 1, 0, 1, 2, 0, 2, 1,\n",
       "       2, 0, 0, 2, 0, 0, 0, 2, 1, 2, 0, 1, 2, 2, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 2, 2, 0, 2, 2, 2, 2, 0, 0, 1, 2, 1, 1, 0, 0, 0, 0,\n",
       "       0, 1, 0, 2, 2, 2, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0], dtype=int8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a98c0b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "class SpectralPatchEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Embeds spectral data into patches with a specified embedding dimension.\n",
    "    \n",
    "    Parameters:\n",
    "        patch_size (int): The size of each patch.\n",
    "        embedding_dim (int): The dimensionality of the embedding output.\n",
    "    \"\"\"\n",
    "    def __init__(self, patch_size: int, embedding_dim :int)-> None:\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding = nn.Linear(patch_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor)-> torch.Tensor:\n",
    "        x = x.unfold(1, self.patch_size, self.patch_size).contiguous()\n",
    "        x = x.view(x.size(0), -1, self.patch_size)\n",
    "        x = self.embedding(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Adds a positional encoding to embedded spectral data.\n",
    "    \n",
    "    Parameters:\n",
    "        d_model (int): The dimensionality of the model's input.\n",
    "        max_len (int, optional): The maximum length of the input sequences. Defaults to 5000.\n",
    "        base (float, optional): The base of the logarithm used in calculating the div_term. Defaults to 10000.0.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, base: float = 10000.0) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.base = base\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(base) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return x\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    '''\n",
    "    FeatureExtractor: Extracts features from spectral data using a Transformer encoder.\n",
    "    '''\n",
    "    def __init__(self, num_features : int , patch_size : int , embedding_dim : int, num_heads : int, num_encoder_layers : int, \n",
    "                 dim_feedforward : int,activation_fn=F.gelu)-> None:\n",
    "        super().__init__()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "        self.patch_embedding = SpectralPatchEmbedding(patch_size, embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, activation=activation_fn)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "    def forward(self, inputs : torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = inputs.size(0)\n",
    "        #print(f'inputs= {inputs.shape}')\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        inputs = self.patch_embedding(inputs) #[batch_size,patches,embedding]\n",
    "        #print(f'after patch_embedding= {inputs.shape}')\n",
    "        inputs = torch.cat((cls_tokens, inputs), dim=1) #[batch_size,patches+ cls token,embedding]\n",
    "        #print(f'after class token= {inputs.shape}')\n",
    "        inputs = self.pos_encoder(inputs)\n",
    "        output = self.transformer_encoder(inputs) #[batch_size,patches+ cls token,embedding]\n",
    "        #print(f'after transformer= {output.shape}')\n",
    "        return output\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    '''\n",
    "    ClassificationHead: Performs classification on features extracted from spectral data.\n",
    "    Incorporates AdaptiveAvgPool1d for pooling over sequence dimension.\n",
    "    '''\n",
    "    def __init__(self, input_features : int, hidden_features: int,embedding_dim: int, num_classes: int\n",
    "                 ,activation_fn=F.gelu)-> None:\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_features, hidden_features)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_features)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.fc3 = nn.Linear(hidden_features, num_classes)\n",
    "        # Initialize AdaptiveAvgPool1d\n",
    "        # Here, output size is set to 1 to pool across all sequence elements\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(int(embedding_dim//2))\n",
    "        self.activation=activation_fn\n",
    "\n",
    "    def forward(self, inputs : torch.Tensor) -> torch.Tensor:\n",
    "        \n",
    "\n",
    "        #print(f'in classification {inputs.shape}')\n",
    "        \n",
    "        # Apply adaptive pooling to reduce seq_len to 1\n",
    "        x = self.adaptive_pool(inputs) \n",
    "        #print(f'in classification after pool {x.shape}')\n",
    "        # Flatten the output for the linear layer\n",
    "        x = torch.flatten(x, 1)  # Now x should be [batch_size, features= embedding//2 * patches * cls token]\n",
    "        # Continue through the classification head\n",
    "        #print(f'in classification after flat {x.shape}')\n",
    "        x = self.activation(self.bn1(self.fc1(x)))\n",
    "       \n",
    "        x = self.dropout(x)\n",
    "        x=self.activation(self.bn1(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        #print(f'in classification after after flat {x.shape}')\n",
    "        # Apply softmax to the output layer for classification probabilities\n",
    "        return F.softmax(x, dim=1) \n",
    "\n",
    "\n",
    "class RegressionHead(nn.Module):\n",
    "    '''\n",
    "    RegressionHead: Performs regression on features extracted from spectral data.\n",
    "    '''\n",
    "    def __init__(self, input_features : int, hidden_features: int,embedding_dim: int, output_features: int,\n",
    "                 activation_fn=F.gelu)-> None:\n",
    "        super(RegressionHead, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_features, hidden_features)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_features)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.fc3 = nn.Linear(hidden_features, output_features)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.activation=activation_fn\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(int(embedding_dim//2))\n",
    "\n",
    "    def forward(self, inputs : torch.Tensor) -> torch.Tensor:\n",
    "        #print(f'inputs 1 shape in regression {inputs.shape}')\n",
    "        x = self.adaptive_pool(inputs) # Assuming global average pooling over sequence dimension\n",
    "        x = torch.flatten(x, 1)\n",
    "        #print(f'x shape in regression {x.shape}')\n",
    "        \n",
    "        x = self.activation(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x= self.activation(self.bn1(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        #print(f'after regression= {x.shape}')\n",
    "        return self.fc3(x)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    '''\n",
    "    CombinedModel: Integrates FeatureExtractor, ClassificationHead, and RegressionHead.\n",
    "    Allows optional bypass of ClassificationHead based on a flag.\n",
    "    '''\n",
    "    def __init__(self, num_features : int, patch_size : int, embedding_dim : int,\n",
    "                 num_heads : int, num_encoder_layers : int, dim_feedforward : int,\n",
    "                 hidden_features_class: int, num_classes: int, hidden_features_reg: int, output_features_reg: int,\n",
    "                 use_classification : bool=True,activation_fn=F.gelu)-> None:\n",
    "        \n",
    "        super().__init__()\n",
    "        self.classification_input=((num_features//patch_size)+1)*(embedding_dim//2) #calculating classification inputs. +1 is the class token and //2 is for adaptive pooling factor\n",
    "        if use_classification:\n",
    "            regression_input= ((num_features//patch_size)+2)*(embedding_dim//2)\n",
    "        else:\n",
    "            regression_input= ((num_features//patch_size)+1)*(embedding_dim//2)\n",
    "            \n",
    "        self.feature_extractor = FeatureExtractor(num_features, patch_size, embedding_dim, num_heads, num_encoder_layers, dim_feedforward,activation_fn=F.gelu)\n",
    "        self.use_classification = use_classification\n",
    "        self.classification_head = ClassificationHead(self.classification_input, hidden_features_class,embedding_dim, num_classes,activation_fn=F.gelu)\n",
    "        self.regression_head = RegressionHead(regression_input,hidden_features_reg,embedding_dim, \n",
    "                                              output_features_reg,activation_fn=F.gelu)\n",
    "        self.adjust_class_score=nn.Linear(num_classes,embedding_dim)\n",
    "        \n",
    "    def forward(self, inputs : torch.Tensor) -> torch.Tensor:\n",
    "        features = self.feature_extractor(inputs)\n",
    "        \n",
    "        if self.use_classification:\n",
    "            class_scores = self.classification_head(features)\n",
    "            #print(features.shape,class_scores.shape)\n",
    "            class_scores_updated=self.adjust_class_score(class_scores)\n",
    "            #print(features.shape,class_scores_updated.unsqueeze(1).shape)\n",
    "            reg_input=torch.cat((features, class_scores_updated.unsqueeze(1)), dim=1) \n",
    "            #reg_input = features * class_scores_updated.unsqueeze(1)\n",
    "            #print(f'regression features {reg_input.shape}')\n",
    "            reg_output = self.regression_head(reg_input)\n",
    "            return class_scores, reg_output\n",
    "        else:\n",
    "            reg_input=features\n",
    "            reg_output = self.regression_head(reg_input)\n",
    "            return None,reg_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5d48065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "class SpectralPatchEmbedding(nn.Module):\n",
    "    def __init__(self, patch_size: int, embedding_dim: int) -> None:\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding = nn.Linear(patch_size, embedding_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x.unfold(1, self.patch_size, self.patch_size).contiguous()\n",
    "        x = x.view(x.size(0), -1, self.patch_size)\n",
    "        x = self.embedding(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_len: int = 5000, base: float = 10000.0) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.base = base\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(base) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return x\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, num_features: int, patch_size: int, embedding_dim: int, num_heads: int, num_encoder_layers: int, dim_feedforward: int, activation_fn=F.gelu) -> None:\n",
    "        super().__init__()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embedding_dim))\n",
    "        self.patch_embedding = SpectralPatchEmbedding(patch_size, embedding_dim)\n",
    "        self.pos_encoder = PositionalEncoding(embedding_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embedding_dim, nhead=num_heads, dim_feedforward=dim_feedforward, activation=activation_fn)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = inputs.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        inputs = self.patch_embedding(inputs)\n",
    "        inputs = torch.cat((cls_tokens, inputs), dim=1)\n",
    "        inputs = self.pos_encoder(inputs)\n",
    "        output = self.transformer_encoder(inputs)\n",
    "        return output\n",
    "\n",
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_features: int, hidden_features: int, embedding_dim: int, num_classes: int, activation_fn=F.gelu) -> None:\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_features, hidden_features)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_features)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.fc3 = nn.Linear(hidden_features, num_classes)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(int(embedding_dim // 2))\n",
    "        self.activation = activation_fn\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.adaptive_pool(inputs)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.activation(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn1(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, input_features: int, hidden_features: int, embedding_dim: int, output_features: int, activation_fn=F.gelu) -> None:\n",
    "        super(RegressionHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_features, hidden_features)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_features)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.fc3 = nn.Linear(hidden_features, output_features)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.activation = activation_fn\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(int(embedding_dim // 2))\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.adaptive_pool(inputs)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.activation(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.bn1(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, num_features: int, patch_size: int, embedding_dim: int, num_heads: int, num_encoder_layers: int, dim_feedforward: int, hidden_features_class: int, num_classes: int, hidden_features_reg: int, output_features_reg: int, use_classification: bool = True, activation_fn=F.gelu) -> None:\n",
    "        super().__init__()\n",
    "        self.classification_input = ((num_features // patch_size) + 1) * (embedding_dim // 2)\n",
    "        regression_input = ((num_features // patch_size) + (2 if use_classification else 1)) * (embedding_dim // 2)\n",
    "\n",
    "        self.feature_extractor = FeatureExtractor(num_features, patch_size, embedding_dim, num_heads, num_encoder_layers, dim_feedforward, activation_fn=F.gelu)\n",
    "        self.use_classification = use_classification\n",
    "        self.classification_head = ClassificationHead(self.classification_input, hidden_features_class, embedding_dim, num_classes, activation_fn=F.gelu)\n",
    "        self.regression_head = RegressionHead(regression_input, hidden_features_reg, embedding_dim, output_features_reg, activation_fn=F.gelu)\n",
    "        self.adjust_class_score = nn.Linear(num_classes, embedding_dim)\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        features = self.feature_extractor(inputs)\n",
    "        if self.use_classification:\n",
    "            class_scores = self.classification_head(features)\n",
    "            class_scores_updated = self.adjust_class_score(class_scores)\n",
    "            reg_input = torch.cat((features, class_scores_updated.unsqueeze(1)), dim=1)\n",
    "            reg_output = self.regression_head(reg_input)\n",
    "            return class_scores, reg_output\n",
    "        else:\n",
    "            reg_output = self.regression_head(features)\n",
    "            return None, reg_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0799e49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLossWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    MultiTaskLossWrapper calculate the weighted loss based on classification scores and regression scores. \n",
    "    Weights are learnable able parameters. \n",
    "    \"\"\"\n",
    "    def __init__(self, model,train_flag=True)-> None:\n",
    "        super(MultiTaskLossWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.classification_loss = nn.CrossEntropyLoss()\n",
    "        self.regression_loss = nn.MSELoss()\n",
    "        # Initialize learnable weights for each loss component\n",
    "        self.task_weights = nn.Parameter(torch.ones(2, dtype=torch.float))\n",
    "        self.flag=train_flag\n",
    "\n",
    "    def forward(self, inputs, targets_classification, targets_regression):\n",
    "        self.model.train(self.flag)\n",
    "        class_scores, reg_output = self.model(inputs)\n",
    "       \n",
    "    \n",
    "        loss_classification = self.classification_loss(class_scores, targets_classification)\n",
    "\n",
    "        loss_regression = self.regression_loss(reg_output, targets_regression)\n",
    "        # Combine losses with learnable weights\n",
    "        #apply softmax to task_weights to ensure they are non-negative and sum to 1\n",
    "#         weights = F.softmax(self.task_weights, dim=0)\n",
    "        \n",
    "#         combined_loss = torch.sum(weights[0] * loss_classification +\n",
    "#                                   weights[1] * loss_regression)\n",
    "        combined_loss=torch.sum(loss_classification + loss_regression)\n",
    "#         print(f'first weight {weights[0]} \\n 2nd weight {weights[1]}')\n",
    "        return combined_loss, loss_classification, loss_regression\n",
    "# class MultiTaskLossWrapper(nn.Module):\n",
    "#     \"\"\"\n",
    "#     MultiTaskLossWrapper calculates the weighted loss based on classification scores and regression scores\n",
    "#     using homoscedastic uncertainty as learnable parameters to weight the losses.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, model, train_flag=True):\n",
    "#         super(MultiTaskLossWrapper, self).__init__()\n",
    "#         self.model = model\n",
    "#         self.classification_loss = nn.CrossEntropyLoss()\n",
    "#         self.regression_loss = nn.MSELoss()\n",
    "#         # Initialize log variance (uncertainty) parameters for each task with uniform distribution\n",
    "#         #self.log_sigma1 = nn.Parameter(torch.rand(1) * (1 - 0.2) + 0.2) # For classification\n",
    "#         #self.log_sigma2 = nn.Parameter(torch.rand(1) * (1 - 0.2) + 0.2) # For regression\n",
    "\n",
    "#         self.log_sigma = nn.Parameter(torch.ones(2, dtype=torch.float))\n",
    "        \n",
    "#         self.train_flag = train_flag\n",
    "\n",
    "#     def forward(self, inputs, targets_classification, targets_regression):\n",
    "#         self.model.train(self.train_flag)\n",
    "#         class_scores, reg_output = self.model(inputs)\n",
    "        \n",
    "#         # Compute classification and regression losses\n",
    "#         loss = [self.classification_loss(class_scores, targets_classification),\n",
    "#                 self.regression_loss(reg_output, targets_regression)]\n",
    "#         loss_sum = 0\n",
    "#         for i, l in enumerate(loss):\n",
    "#             loss_sum += (0.5 / (self.log_sigma[i] ** 2)) * l + torch.log(1 + self.log_sigma[i] ** 2)\n",
    "#         #print(f'Classification Loss Weight: {self.log_sigma[0]}, Regression Loss Weight: {self.log_sigma[1]}')\n",
    "#         return loss_sum,(0.5 / (self.log_sigma** 2)),loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94d4acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### \n",
    "num_features = X_train.shape[1]  # Number of features in each sample\n",
    "patch_size = 8 # Arbitrary choice for the sake of example\n",
    "embedding_dim = 64  # Embedding dimension after patch embedding\n",
    "num_heads = 4  # Number of attention heads in Transformer encoder\n",
    "num_encoder_layers = 8 # Number of layers in Transformer encoder\n",
    "dim_feedforward = 128 # Dimension of feedforward network in Transformer encoder\n",
    "hidden_features_class =64  # Hidden layer size for  (num_features//patch_size + cls token) * (embedding_dim got half due to adp )\n",
    "num_classes = 3  # Number of classes for classification\n",
    "hidden_features_reg = 64  # Hidden layer size for regression head\n",
    "output_features_reg = 1  # Output size for regression (single value prediction)\n",
    "use_classification = True # Flag to use classification head\n",
    "batch_size=16\n",
    "# Initialize the combined model\n",
    "combined_model = CombinedModel(num_features, patch_size, embedding_dim, num_heads, num_encoder_layers,\n",
    "                               dim_feedforward, hidden_features_class, num_classes, hidden_features_reg,\n",
    "                               output_features_reg, use_classification)\n",
    "loss_wrapper= MultiTaskLossWrapper(combined_model)\n",
    "\n",
    "optimizer = optim.Adam(combined_model.parameters(), lr=0.001)  # Default weight decay for all parameters not explicitly set\n",
    "\n",
    "\n",
    "\n",
    "# Create a TensorDataset\n",
    "train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train_class).long(), torch.from_numpy(y_train_reg.values).float())\n",
    "#train_dataset = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train_reg.values).float())\n",
    "\n",
    "val_dataset = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test_class).long(), torch.from_numpy(y_test_reg.values).float())\n",
    "\n",
    "#val_dataset = TensorDataset(torch.from_numpy(X_test).float(),  torch.from_numpy(y_test_reg.values).float())\n",
    "\n",
    "# Create a DataLoader\n",
    "training_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                              batch_size=batch_size,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=1)\n",
    "\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(val_dataset,\n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=False,\n",
    "                                                num_workers=2)\n",
    "writer = SummaryWriter('runs/actual3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8989442",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9484\\4062409627.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mcombined_loss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_class\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mloss_reg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_class_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_reg_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mloss_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mloss_reg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             )\n\u001b[1;32m--> 488\u001b[1;33m         torch.autograd.backward(\n\u001b[0m\u001b[0;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         )\n",
      "\u001b[1;32mC:\\tools\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 197\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "for epoch in range(100):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(training_loader, 0):\n",
    "        # basic training loop\n",
    "        x_batch, y_class_batch, y_reg_batch = data\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "#         combined_loss,weights,separate_loss = loss_wrapper(x_batch, y_class_batch, y_reg_batch)\n",
    "    \n",
    "        combined_loss,loss_class,loss_reg = loss_wrapper(x_batch, y_class_batch, y_reg_batch)\n",
    "        loss_class.backward()\n",
    "        loss_reg.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        running_loss += loss_reg.item()\n",
    "        \n",
    "        if i % 10 == 0:    # Every 10 mini-batches...\n",
    "#             print('Batch {}'.format(i + 1))\n",
    "            # Check against the validation set\n",
    "            running_vloss = 0.0\n",
    "            loss_wrapper.train_flag=False\n",
    "             # Don't need to track gradents for validation\n",
    "            for j, vdata in enumerate(validation_loader, 0):\n",
    "                vx_batch, vy_class_batch, vy_reg_batch = vdata\n",
    "                vcombined_loss,vloss_class,vloss_reg= loss_wrapper(vx_batch, vy_class_batch, vy_reg_batch)\n",
    "                running_vloss += vloss_reg.item()\n",
    "                \n",
    "            loss_wrapper.train_flag=True\n",
    "            \n",
    "            avg_loss = running_loss / 1000\n",
    "            avg_vloss = running_vloss / len(validation_loader)\n",
    "            \n",
    "            # Log the running loss averaged per batch\n",
    "            writer.add_scalars('Training vs. Validation Loss',\n",
    "                            { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                            epoch * len(training_loader) + i)\n",
    "\n",
    "            running_loss = 0.0\n",
    "#             if device.type == 'cuda':\n",
    "#                 print(torch.cuda.get_device_name(0))\n",
    "#                 print('Memory Usage:')\n",
    "#                 print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**2,1), 'GB')\n",
    "#                 print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**2,1), 'GB')\n",
    "    #print(f'Epoch {epoch}----train loss {avg_loss}---val loss {avg_vloss}, \\n weights_cls {weights[0]} weights_reg {weights[1]}')\n",
    "    print(f'Epoch {epoch}----train loss {avg_loss}---val loss {avg_vloss}')\n",
    "print('Finished Training')\n",
    "\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d9f7a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aad63d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Inception_model import Inception\n",
    "from utils import init_weights,Make_one_one_plot,benchmark,train_One_epoch,evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9be02232",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Inception()\n",
    "model = model.apply(init_weights)\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.00316) \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0002633, max_lr=0.00316,step_size_up=3664, \n",
    "                                              step_size_down=None, mode='exp_range',gamma=0.99994,scale_fn=None,\n",
    "                                              cycle_momentum=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b65df45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] Training loss: 1.937\t Validation loss: 1.479\n",
      "New Best model found\n",
      "[2] Training loss: 1.792\t Validation loss: 1.296\n",
      "New Best model found\n",
      "[3] Training loss: 1.570\t Validation loss: 1.444\n",
      "[4] Training loss: 1.713\t Validation loss: 1.912\n",
      "[5] Training loss: 1.344\t Validation loss: 1.426\n",
      "[6] Training loss: 1.385\t Validation loss: 1.311\n",
      "[7] Training loss: 1.443\t Validation loss: 1.703\n",
      "[8] Training loss: 1.799\t Validation loss: 1.388\n",
      "[9] Training loss: 1.969\t Validation loss: 1.815\n",
      "[10] Training loss: 1.651\t Validation loss: 1.739\n",
      "[11] Training loss: 1.407\t Validation loss: 1.677\n",
      "[12] Training loss: 1.416\t Validation loss: 1.458\n",
      "[13] Training loss: 1.518\t Validation loss: 1.469\n",
      "[14] Training loss: 1.468\t Validation loss: 1.222\n",
      "New Best model found\n",
      "[15] Training loss: 1.653\t Validation loss: 1.978\n",
      "[16] Training loss: 1.367\t Validation loss: 1.436\n",
      "[17] Training loss: 1.337\t Validation loss: 1.163\n",
      "New Best model found\n",
      "[18] Training loss: 1.302\t Validation loss: 1.312\n",
      "[19] Training loss: 1.245\t Validation loss: 1.328\n",
      "[20] Training loss: 1.258\t Validation loss: 1.205\n",
      "[21] Training loss: 1.204\t Validation loss: 1.212\n",
      "[22] Training loss: 1.270\t Validation loss: 1.193\n",
      "[23] Training loss: 1.327\t Validation loss: 1.241\n",
      "[24] Training loss: 1.160\t Validation loss: 1.871\n",
      "[25] Training loss: 1.350\t Validation loss: 1.385\n",
      "[26] Training loss: 1.298\t Validation loss: 1.584\n",
      "[27] Training loss: 1.195\t Validation loss: 1.835\n",
      "[28] Training loss: 1.320\t Validation loss: 1.178\n",
      "[29] Training loss: 1.308\t Validation loss: 1.206\n",
      "[30] Training loss: 1.448\t Validation loss: 1.174\n",
      "[31] Training loss: 1.633\t Validation loss: 1.413\n",
      "[32] Training loss: 1.390\t Validation loss: 1.276\n",
      "[33] Training loss: 1.232\t Validation loss: 1.415\n",
      "[34] Training loss: 1.375\t Validation loss: 2.577\n",
      "[35] Training loss: 1.756\t Validation loss: 1.715\n",
      "[36] Training loss: 1.364\t Validation loss: 1.329\n",
      "[37] Training loss: 1.312\t Validation loss: 1.302\n",
      "[38] Training loss: 1.146\t Validation loss: 1.427\n",
      "[39] Training loss: 1.097\t Validation loss: 1.241\n",
      "[40] Training loss: 1.146\t Validation loss: 1.201\n",
      "[41] Training loss: 1.240\t Validation loss: 1.180\n",
      "[42] Training loss: 1.164\t Validation loss: 1.343\n",
      "[43] Training loss: 1.238\t Validation loss: 1.559\n",
      "[44] Training loss: 1.167\t Validation loss: 1.306\n",
      "[45] Training loss: 1.137\t Validation loss: 1.137\n",
      "New Best model found\n",
      "[46] Training loss: 1.139\t Validation loss: 1.279\n",
      "[47] Training loss: 1.216\t Validation loss: 1.234\n",
      "[48] Training loss: 1.200\t Validation loss: 1.213\n",
      "[49] Training loss: 1.218\t Validation loss: 1.474\n",
      "[50] Training loss: 1.212\t Validation loss: 1.819\n",
      "[51] Training loss: 1.182\t Validation loss: 1.485\n",
      "[52] Training loss: 1.148\t Validation loss: 1.439\n",
      "[53] Training loss: 1.141\t Validation loss: 1.546\n",
      "[54] Training loss: 1.195\t Validation loss: 1.233\n",
      "[55] Training loss: 1.203\t Validation loss: 1.354\n",
      "[56] Training loss: 1.196\t Validation loss: 1.355\n",
      "[57] Training loss: 1.176\t Validation loss: 1.387\n",
      "[58] Training loss: 1.230\t Validation loss: 1.391\n",
      "[59] Training loss: 1.224\t Validation loss: 1.303\n",
      "[60] Training loss: 1.241\t Validation loss: 1.420\n",
      "[61] Training loss: 1.177\t Validation loss: 1.459\n",
      "[62] Training loss: 1.105\t Validation loss: 1.464\n",
      "[63] Training loss: 1.140\t Validation loss: 1.141\n",
      "[64] Training loss: 1.217\t Validation loss: 1.149\n",
      "[65] Training loss: 1.184\t Validation loss: 3.201\n",
      "[66] Training loss: 1.509\t Validation loss: 2.276\n",
      "[67] Training loss: 1.305\t Validation loss: 1.281\n",
      "[68] Training loss: 1.368\t Validation loss: 1.336\n",
      "[69] Training loss: 1.452\t Validation loss: 1.523\n",
      "[70] Training loss: 1.185\t Validation loss: 1.627\n",
      "[71] Training loss: 1.180\t Validation loss: 1.407\n",
      "[72] Training loss: 1.168\t Validation loss: 1.312\n",
      "[73] Training loss: 1.242\t Validation loss: 1.436\n",
      "[74] Training loss: 1.238\t Validation loss: 1.469\n",
      "[75] Training loss: 1.166\t Validation loss: 1.309\n",
      "[76] Training loss: 1.145\t Validation loss: 1.342\n",
      "[77] Training loss: 1.233\t Validation loss: 1.178\n",
      "[78] Training loss: 1.227\t Validation loss: 1.235\n",
      "[79] Training loss: 1.090\t Validation loss: 1.254\n",
      "[80] Training loss: 1.109\t Validation loss: 1.246\n",
      "[81] Training loss: 1.153\t Validation loss: 1.399\n",
      "[82] Training loss: 1.156\t Validation loss: 1.260\n",
      "[83] Training loss: 1.086\t Validation loss: 1.267\n",
      "[84] Training loss: 1.102\t Validation loss: 1.150\n",
      "[85] Training loss: 1.188\t Validation loss: 1.248\n",
      "[86] Training loss: 1.128\t Validation loss: 1.264\n",
      "[87] Training loss: 1.088\t Validation loss: 1.299\n",
      "[88] Training loss: 1.215\t Validation loss: 1.852\n",
      "[89] Training loss: 1.227\t Validation loss: 1.526\n",
      "[90] Training loss: 1.154\t Validation loss: 1.159\n",
      "[91] Training loss: 1.158\t Validation loss: 1.248\n",
      "[92] Training loss: 1.227\t Validation loss: 1.220\n",
      "[93] Training loss: 1.090\t Validation loss: 1.324\n",
      "[94] Training loss: 1.064\t Validation loss: 1.373\n",
      "[95] Training loss: 1.078\t Validation loss: 1.345\n",
      "[96] Training loss: 1.138\t Validation loss: 1.243\n",
      "[97] Training loss: 1.162\t Validation loss: 1.564\n",
      "[98] Training loss: 1.100\t Validation loss: 1.331\n",
      "[99] Training loss: 1.122\t Validation loss: 1.414\n",
      "[100] Training loss: 1.184\t Validation loss: 1.201\n"
     ]
    }
   ],
   "source": [
    "training_losses=[]\n",
    "training_lrs=[]\n",
    "validation_losses=[]\n",
    "best_valid=1000\n",
    "for epoch in range(100):\n",
    "    training_loss,lrs=train_One_epoch(model,training_loader,optimizer,loss_fn)  \n",
    "    training_losses.append(training_loss)\n",
    "    training_lrs.append(lrs)\n",
    "    #Evaluate the model\n",
    "    validation_loss=evaluate(model,val_loader,loss_fn)\n",
    "    validation_losses.append(validation_loss)\n",
    "\n",
    "    print(f\"[{epoch+1}] Training loss: {training_loss:.3f}\\t Validation loss: {validation_loss:.3f}\")\n",
    "    \n",
    "    #Check the losses and save the model\n",
    "    if validation_loss < best_valid:\n",
    "        best_valid = validation_loss\n",
    "        print(\"New Best model found\")\n",
    "#         torch.save({\n",
    "#                 'epoch': epoch,\n",
    "#                 'model_state_dict': model.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'train_loss':training_loss,\n",
    "#                 'val_loss':validation_loss\n",
    "#                 },f\"../Deep learning approach/Weights/test_wts.pt\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dffc991",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e0e1cc48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31308\\769695013.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchviz\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmake_dot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torchviz'"
     ]
    }
   ],
   "source": [
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c01a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchviz in c:\\tools\\anaconda3\\envs\\blueberry\\lib\\site-packages (0.0.2)\n",
      "Requirement already satisfied: torch in c:\\tools\\anaconda3\\envs\\blueberry\\lib\\site-packages (from torchviz) (2.1.0)\n",
      "Requirement already satisfied: graphviz in c:\\tools\\anaconda3\\envs\\blueberry\\lib\\site-packages (from torchviz) (0.20.3)\n",
      "Requirement already satisfied: filelock in c:\\tools\\anaconda3\\envs\\blueberry\\lib\\site-packages (from torch->torchviz) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\tools\\anaconda3\\envs\\blueberry\\lib\\site-packages (from torch->torchviz) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\tools\\anaconda3\\envs\\blueberry\\lib\\site-packages (from torch->torchviz) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\tools\\anaconda3\\envs\\blueberry\\lib\\site-packages (from torch->torchviz) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\tools\\anaconda3\\envs\\blueberry\\lib\\site-packages (from torch->torchviz) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\tools\\anaconda3\\envs\\blueberry\\lib\\site-packages (from torch->torchviz) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\tools\\anaconda3\\envs\\blueberry\\lib\\site-packages (from jinja2->torch->torchviz) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\tools\\anaconda3\\envs\\blueberry\\lib\\site-packages (from sympy->torch->torchviz) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1721e68f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651eb695",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
